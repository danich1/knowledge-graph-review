## Building Biomedical Knowledge Graphs

Knowledge graphs can be constructed in many ways using resources such as pre-exisitng databases or text.
Usually, knowledge graphs are constructed using pre-existing databases and these databases are constructed by domain experts using approaches ranging from manual curation to automated techniques, such as text mining.
Manual curation is a time consuming process that requires domain experts to read papers and annotate sentences that assert a relationship.
Automated approaches rely on machine learning or natural language processing techniques to rapidly detect sentences of interest.
We categorize these automated approaches into the following groups: rule-based extraction, unsupervised machine learning, and supervised machine learning and discuss examples of each type of approach while synthesizing their strengths and weaknesses.

### Constructing Databases and Manual Curation

Database construction can date back all the way to 1956 where the first database contained a protein sequence of the insulin molecule [@doi:10.4172/jcsb.1000081].
This process involves gathering relevant text such as journal articles, abstracts, or web-based text and having curators read the gathered text to detect sentences that implicate a relationship (i.e. relationship extraction).
Notable databases constructed by this process can be in found in Table {@tbl:manual-curated-databases}.
An example database, COSMIC [@doi:10.1093/nar/gkw1121] was constructed via a group of domain experts scanning the literature for key cancer related genes.
This database contained approximately 35M entries in 2016 [@doi:10.1093/nar/gkw1121] and by 2019 had grown to 45M entries [@doi:10.1093/nar/gky1015].
Studies have shown that databases constructed in this fashion contain relatively precise data, but in low quantifies [@doi:10.1038/nmeth1209-860; @doi:10.1038/nmeth.1284; @doi:10.1093/database/bau058; @doi:10.1093/nar/gku1205; @doi:10.1186/s12859-018-2103-8; @doi:10.1093/database/bax043; @doi:10.1093/bioinformatics/btm229].
This happens because the publication rate is too high for curators to keep up [@doi:10.1007/s11192-010-0202-z].
This bottleneck highlights a critical need for future approaches to scale fast enough to compete with the increasing publication rate.

Semi-automatic methods are a way to accelerate the curation process [@doi:10.1016/j.jbi.2010.11.001; @doi:10.1093/database/bau067; @doi:10.1093/database/bas010; @doi:10.1145/3269206.3269229; @doi:10.1093/database/baz068; @doi:10.1186/s12859-018-2103-8; @doi:10.1186/s12859-018-2021-9].
The first step of these methods is to use an automated system to initially extract sentences from text.
This process removes irrelevant sentences, which dramatically decreases the amount of text that curators must sift through.
Following the pre-filtering step, curators then approve or reject the remaining sentences.
This approach saved curators an average of 2-2.8 hours compared to manual efforts [@doi:10.1016/j.jbi.2010.11.001; @doi:10.1136/amiajnl-2013-001837]. 
Despite automated systems excelling in identifying sentences for commonly occurring relationships, they tend to miss lessor known relationships [@doi:10.1016/j.jbi.2010.11.001].
These systems also have a hard time parsing ambiguous sentences that naturally occur in text, which makes correcting them a challenging task [@doi:10.1016/j.jbi.2010.11.001].
Given these caveats, future approaches should look into using techniques that simplify sentences to solve the ambiguity issue [@doi:10.1093/database/bau038; @pmid:21346999].

Despite the negatives of manual curation, it is still an essential process for extracting relationships from text.
This process can be used to generate gold standard datasets that automated systems use for validation [@doi:10.1016/j.jbi.2012.04.004; @doi:10.1016/j.artmed.2004.07.016] and can be used during the training process of these systems (i.e. active learning) [@doi:10.1007/s11390-012-1306-0].
It is important to remember that manual curation alone is precise, but results in low recall rates [@doi:10.1093/bioinformatics/btm229].
Future databases should consider initially relying on automated methods to obtain sentences at an acceptable recall level, then incorporate manual curation as a way to fix or remove irrelevant results.

| Database [Reference] | Short Description | Number of Entries | Entity  Types | Relationship Types | Method of Population |
| --- | --- | --- | --- | --- | --- |
| BioGrid [@doi:10.1093/nar/gks1158] | A database for major model organisms. It contains genetic and proteomic information.| 572,084 | Genes, Proteins| Protein-Protein interactions | Semi-automatic methods |
| Comparative Toxicogenomics Database [@doi:10.1093/nar/gky868] | A database that contains manually curated chemical-gene-disease interactions and  relationships. | 2,429,689 | Chemicals (Drugs), Genes, Diseases | Drug-Genes, Drug-Disease, Disease-Gene mappings | Manual curation and Automated systems |
| Comprehensive Antibiotic Resistance Database [@doi:10.1093/nar/gkw1004] | Manually curated database that contains information about the molecular basis of antimicrobial resistance. | 174,443 | Drugs, Genes, Variants | Drug-Gene, Drug-Variant mappings | Manual curation |
| COSMIC [@doi:10.1093/nar/gkw1121] | A database that contains high resolution human cancer genetic information. | 35,946,704 | Genes, Variants, Tumor Types| Gene-Variant Mappings | Manual Curation |
| Entrez-Gene [@doi:10.1093/nar/gkq1237] | NCBI's Gene annotation database that contains information pertaining to genes, gene's organism source, phenotypes etc. | 7,883,114 | Genes, Species and Phenotypes | Gene-Phenotypes and Genes-Species mappings | Semi-automated curation |
| OMIM [@pmid:30445645] | A database that contains phenotype and genotype information | 25,153 | Genes, Phenotypes | Gene-Phenotype mappings | Manual Curation |
| PharmGKB [@doi:10.1038/clpt.2012.96] | A database that contains genetic, phenotypic, and clinical information related to pharmacogenomic studies. | 43,112 | Drugs, Genes, Phenotypes, Variants, Pathways | Gene-Phenotypes, Pathway-Drugs, Gene-Variants, Gene-Pathways | Manual Curation and Automated Methods |
| UniProt [@doi:10.1093/nar/gky1049] | A protein protein interaction database that contains proteomic information. | 560,823 | Proteins, Protein sequences | Protein-Protein interactions | Manual and Automated Curation | 

Table: A table of databases that used a form of manual curation to populate entries. 
Reported number of entities and relationships are relative to time of publication.
{#tbl:manual-curated-databases}

### Text Mining for Relationship Extraction

#### Rule-Based Relationship Extraction

Rule based-extraction consists of identifying essential keywords and grammatical patterns to detect relationships of interest. 
Keywords are established via expert knowledge or though the use of pre-existing ontologies, while grammatical patterns are constructed via experts curating parse trees. 
Parse trees are tree data structures that depict a sentence's grammatical structure and come into two forms: a constituency parse tree (Figure {@fig:constituency-parse-tree-example}) and a dependency parse tree (Figure {@fig:dependency-parse-tree-example}).
Both trees use part of speech tags, labels that dictate the grammatical role of a word such as noun, verb, adjective, etc, for construction, but represent the information in two different forms.
Constituency parse trees breaks a sentence down into a subphrases (Figure {@fig:constituency-parse-tree-example}) while dependency path trees analyzes the grammatical structure of a sentence (Figure {@fig:dependency-parse-tree-example}).
Many text mining approaches [@doi:10.1093/database/bay108; @doi:10.1093/bioinformatics/btw503; @doi:10.1186/s13326-017-0168-3] use such trees to generate features for machine learning algorithms and these approaches are discussed in later sections.
In this section we focus on approaches that use rule based extraction as a primary strategy to detect sentences that allude to a relationship.

Grammatical patterns can simplify sentences for easy extraction [@pmid:21346999; @pmid:24850848].
Jonnalagadda et al. used a set of grammar rules inspired by constituency trees to reshape complex sentences with simpler versions [@pmid:21346999] and these simplified versions were manually curated to determine the presence of a relationship.
By simplifying sentences this approach achieved high recall, but had low precision [@pmid:21346999].
Other approach used simplification techniques to make extraction easier [@doi:10.1093/database/baw156; @doi:10.1186/1471-2105-15-285; @doi:10.1093/database/bav020; @doi:10.1371/journal.pcbi.1004391].
Tudor et al. simplified sentences to detect protein phosphorylation events [@doi:10.1093/database/bav020].
Their sentence simplifier broke complex sentences that contain multiple protein events into smaller sentences that contain only one distinct event.
By breaking these sentences down the authors were able to increase their recall; however, sentences that contained ambiguous directionality or multiple phosphorylation events were too complex for the simplifier.
As a consequence the simplifier missed some relevant sentences [@doi:10.1093/database/bav020].
These errors highlight a crucial need for future algorithms to be generalizable enough to handle various forms of complex sentences.

Pattern matching is a fundamental approach used to detect relationship asserting sentences.
These patterns can consist of phrases from constituency trees, a set of keywords or some combination of both [@doi:10.1093/nar/gkx462; @doi:10.1186/s12859-018-2103-8; @doi:10.1371/journal.pone.0152725; @doi:10.1093/bioinformatics/btg449; @doi:10.1186/1471-2105-14-181; @doi:10.1109/TCBB.2014.2372765].
Xu et al. designed a pattern matcher system to detect sentences in PubMed abstracts that indicate drug-disease treatments [@doi:10.1186/1471-2105-14-181].
This system matched drug-disease pairs from ClinicalTrials.gov to drug-disease pairs mentioned in abstracts.
This matching process aided the authors in identifying sentences that can be used to create simple patterns, such as "Drug in the treatment of Disease" [@doi:10.1186/1471-2105-14-181], to match other sentences in a wide variety of abstracts.
The authors hand curated two datasets for evaluation and achieved a high precision score of 0.904 and a low recall score of 0.131 [@doi:10.1186/1471-2105-14-181].
This low recall score was based on constructed patterns being too specific to detect infrequent drug pairs.
Besides constituency trees, some approaches used dependency trees to construct patterns [@doi:10.1016/j.jbi.2015.08.008; @doi:10.1093/database/bay108].
Depending upon the nature of the algorithm and text, dependency trees could be more appropriate than constituency trees and vise versa.
The performance difference between the two trees remains as an open question for future exploration.

Rules based methods provide a basis for many relationship extraction systems.
Approaches in this category range from simplifying sentences for easy extraction to identifying sentences based on matched key phrases or grammatical patterns.
Both require a significant amount of manual effort and expert knowledge to perform well.
A future direction is to develop ways to automatically construct these hand-crafted patterns, which would accelerate the process of creating these rule-based systems.

![
A visualization of a constituency parse tree using the following sentence: "BRCA1 is associated with breast cancer" [@raw:eisenbach2006phpsyntaxtree].
This type of tree has the root start at the beginning of the sentence.
Each word is grouped into subphrases depending its correlating part of speech tag.
For example, the word "associated" is a past participle verb (VBN) that belongs to the verb phrase (VP) subgroup.
](images/figures/constituency_parse_tree_example.png){#fig:constituency-parse-tree-example}

![
A visualization of a dependency parse tree using the following sentence: "BRCA1 is associated with breast cancer" [@raw:honnibal2017spacy].
For these type of trees the root begins with the main verb of the sentence.
Each arrows represents the dependency shared between two words.
For example, the dependency between BRCA1 and associated is nsubjpass, which stands for passive nominal subject.
This means that "BRCA1" is the subject of the sentence and it is being referred to by the word "associated".
](images/figures/dependency_parse_example.png){#fig:dependency-parse-tree-example}

#### Extracting Relationships Without Labels

Unsupervised extractors draw inferences from textual data without the use of annotated labels.
These methods involve some form of clustering or statistical calculations.
In this section we focus on methods that use unsupervised learning to extract relationships from text. 

An unsupervised extractor can exploit the fact that two entities may appear together in text.
This event is referred to as co-occurrence and studies that use this phenomenon can be found in Table {@tbl:unsupervised-methods-text-mining}.
Two databases DISEASES [@doi:10.1016/j.ymeth.2014.11.020] and STRING [@doi:10.1093/nar/gks1094] were populated using a co-occurrence scoring method on PubMed abstracts, which measured the frequency of co-mention pairs within individual sentences as well as the abstracts themselves.
This technique assumes that each individual co-occurring pair is independent from one another.
Under this assumption mention pairs that occur more than expected were presumed to implicate the presence of an association or interaction.
This approach was able to identify 543,405 disease gene associations [@doi:10.1016/j.ymeth.2014.11.020] and 792,730 high confidence protein protein interactions [@doi:10.1093/nar/gks1094], but is limited to only using PubMed abstracts.

Full text articles are able to dramatically enhance relationship detection [@doi:10.1371/journal.pcbi.1005962; @doi:10.1093/nar/gkt1207].
Westergaard et al. used a co-occurrence approach, similar to DISEASES [@doi:10.1016/j.ymeth.2014.11.020] and STRING [@doi:10.1093/nar/gks1094], to mine full articles for protein-protein interactions and other protein related information [@doi:10.1371/journal.pcbi.1005962].
The authors discovered that full text provided better prediction power than using abstracts alone, which suggests that future text mining approaches should consider using full text to increase detection power.

Unsupervised extractors often treat different biomedical relationships as multiple isolated problems.
An alternative to this perspective is to capture all different types at once.
Clustering is an approach that accomplish this concept of simultaneous extraction.
Percha et al. used a biclustering algorithm on generated dependency parse trees to group sentences within PubMed abstract [@doi:10.1093/bioinformatics/bty114].
Each cluster was manually curated to determine which relationship each group represented.
This approach captured 4,451,661 dependency paths for 36 different groups [@doi:10.1093/bioinformatics/bty114].
Despite the success, this approach suffered from technical issues such as dependency tree parsing errors.
These errors resulted in some sentences not being captured by the clustering algorithm [@doi:10.1093/bioinformatics/bty114] and future clustering approaches should consider simplifying sentences to prevent this type of issue.

Overall unsupervised methods provide a means to rapidly extract relationship asserting sentences without the need of annotated text.
Approaches in this category range from calculating co-occurrence scores to clustering sentences and provide a generalizable framework that can be used on large repositories of text.
Full text has already been show to meaningfully improve the performance of methods that aim to infer relationships using cooccurrences [@doi:10.1371/journal.pcbi.1005962], and we should expect similar benefits for machine learning approaches.
Furthermore, we expect that simplifying sentences would improve unsupervised methods and should considered as an initial preprocessing step.

| Study | Relationship of Interest | 
| --- | --- | 
| CoCoScore [@doi:10.1093/bioinformatics/btz490] | Protein-Protein Interactions, Disease-Gene and Tissue-Gene Associations |
| Rastegar-Mojarad et al. [@doi:10.1109/bibm.2015.7359766] | Drug Disease Treatments |
| CoPub Discovery [@doi:10.1371/journal.pcbi.1000943] | Drug, Gene and Disease interactions |
| Westergaard et al. [@doi:10.1371/journal.pcbi.1005962]| Protein-Protein Interactions |
| DISEASES [@doi:10.1016/j.ymeth.2014.11.020] | Disease-Gene associations|
| STRING [@doi:10.1093/nar/gku1003]| Protein-Protein Interactions |
| Singhal et al. [@doi:10.1371/journal.pcbi.1005017] | Genotype-Phenotype Relationships |

Table: Table of approaches that mainly use a form of co-occurrence. {#tbl:unsupervised-methods-text-mining}

#### Supervised Relationship Extraction

Supervised extractors use labeled sentences to construct generalized patterns that bisect positive examples (sentences that allude to a relationship) from negative ones (sentences that do not allude to a relationship).
Most of these approaches have flourished due to pre-labelled publicly available datasets (Table {@tbl:supervised-text-datasets}).
These datasets were constructed by curators for shared open tasks [@raw:biocreative/chemprot; @doi:10.1093/database/baw068] or as a means to provide the scientific community with a gold standard [@doi:10.1093/bioinformatics/btl616; @doi:10.1093/database/baw068; @doi:10.1186/1471-2105-14-323].
Approaches that use these available datasets range from using linear classifiers such as support vector machines (SVMs) to non-linear classifiers such as deep learning techniques.
The rest of this section discuss approaches that use supervised extractors to detect relationship asserting sentences.

Some supervised extractors involve mapping textual input onto a high dimensional space.
SVMs are a type of classifier that can accomplish this task with a mapping function called a kernel [@doi:10.1186/s13326-017-0168-3; @doi:10.1371/journal.pcbi.1004630].
These kernels take information such as a sentence's dependency tree [@doi:10.1093/database/bay108; @doi:10.1093/bioinformatics/btw503], part of speech tags [@doi:10.1186/s13326-017-0168-3] or even word counts [@doi:10.1371/journal.pcbi.1004630] and map them onto a dense feature space.
Within this space, these methods construct a hyperplane that separates sentences in the positive class (illustrates a relationship) from the negative class (does not illustrate a relationship). 
Kernels can be manually constructed or selected to cater to the relationship of interest [@doi:10.1186/s13326-017-0168-3; @doi:10.1093/bioinformatics/btw503;@doi:10.1371/journal.pcbi.1004630; @doi:10.1371/journal.pcbi.1004630].
Determining the correct kernel is a nontrivial task that requires expert knowledge to be successful.
In addition to single kernel methods, a recent study used an ensemble of SVMs to extract disease-gene associations [@doi:10.1371/journal.pone.0200699].
This ensemble outperformed notable disease-gene association extractors [@doi:10.1016/j.jbi.2015.08.008; @doi:10.1186/s12859-015-0472-9] in terms of precision, recall and F1 score.
Overall, SVMs have been shown to be beneficial in terms of relationship mining; however, major focus have shifted to utilizing deep learning techniques as these approaches can perform non-linear mappings of high dimensional data.

Deep learning is an increasingly popular class of techniques that can construct their own features within a high dimensional space [@raw:GoodfellowDL; @doi:10.1038/nature14539].
These methods amount to different forms of neural networks, such as recurrent or convolutional neural networks, to perform classification.

Recurrent neural networks (RNN) are designed for sequential analysis and use a repeatedly updating hidden state to make predictions.
An example of a recurrent neural network is a long short term memory (LSTM) network [@doi:10.1162/neco.1997.9.8.1735].
Cocos et al. [@doi:10.1093/jamia/ocw180] used a LSTM to extract drug side effects from de-identified twitter posts, while Yadav et al. [@doi:10.3233/978-1-61499-830-3-644] used an LSTM to extract protein-protein interactions.
Others have also embraced LSTMs to perform relationship extraction [@arxiv:1708.03743; @doi:10.1093/bioinformatics/btw486; @arxiv:1808.09101; @doi:10.1186/s12859-017-1609-9; @doi:10.1093/jamia/ocw180]. 
Despite the success of these networks, training can be difficult as these networks are highly susceptible to vanishing and exploding gradients [@doi:10.1109/ICNN.1993.298725; @doi:10.1016/j.physd.2019.132306].
One proposed solution to this problem is to clip the gradients while the neural network trains [@arxiv:1211.5063].
Besides the gradient problem, these approaches only peak in performance when the datasets reach at least a tens of thousand of data points [@arxiv:1707.02968].

Convolutional neural networks (CNNs), which are widely applied for image analysis, use multiple kernel filters to capture small subsets of an overall image [@doi:10.1038/nature14539].
In the context of text mining an image is replaced with words within a sentence mapped to dense vectors (i.e., word embeddings) [@arxiv:1301.3781; @arxiv:1310.4546].
Peng et al. used a CNN to extract sentences that mentioned protein-protein interactions [@arxiv:1706.01556v2] and Zhou et al. used a CNN to extract chemical-disease relations [@doi:10.1186/s12859-019-2873-7].
Others have used CNNs and variants of CNNs to extract relationships from text [@doi:10.1177/0165551516673485; @doi:10.1093/database/bay073; @doi:10.1101/730085].
Just like RNNs, these networks perform well when millions of labeled examples are present [@arxiv:1707.02968]; however, obtaining these large datasets is a non-trivial task.
Future approaches that use CNNs or RNNs should consider solutions to obtaining these large quantities of data through means such as weak supervision [@doi:10.3115/1690219.1690287], semi-supervised learning [@doi:10.2200/S00196ED1V01Y200906AIM006] or using pre-trained networks via transfer learning [@doi:10.1109/TKDE.2009.191; @doi:10.1186/s40537-016-0043-6].

Semi-supervised learning [@doi:10.2200/S00196ED1V01Y200906AIM006] and weak supervision [@doi:10.3115/1690219.1690287] are techniques that can rapidly construct large datasets for machine learning classifiers. 
Semi-supervised learning trains classifiers by combining labeled data with unlabeled data.
For example, one study used a variational auto encoder with a LSTM network to extract protein-protein interactions from PubMed abstracts and full text [@arxiv:1901.06103v1].
This is an elegant solution for the small dataset problem, but requires labeled data to start. 
This dependency makes finding under-studied relationships difficult as one would need to find or construct examples of the missing relationships at the start.

Weak or distant supervision takes a different approach that uses noisy or even erroneous labels to train classifiers [@doi:10.3115/1690219.1690287; @doi:10.1093/bioinformatics/btv476; @doi:10.14778/3157794.3157797; @doi:10.1145/3209889.3209898].
Under this paradigm sentences are labeled based on their mention pair being present (positive) or absent (negative) in a database and, once labeled, machine learning classifier can now be trained to extract relationships from text [@doi:10.3115/1690219.1690287].
For example, Thomas et al. [@raw:Thomas2011LearningTE] used distant supervision to train a SVM to extract sentences mentioning protein-protein interactions (ppi). 
Their SVM model achieved comparable performance against a baseline model; however, the noise generated via distant supervision was difficult to eradicate [@raw:Thomas2011LearningTE].
A number of efforts have focused on combining distant supervision with other types of labeling strategies to mitigate the negative impacts of noisy knowledge bases [@arxiv:1805.09927; @arxiv:1805.09929; @doi:10.18653/v1/W17-2323].
Nicholson et al. [@doi:10.1101/730085] found that, in some circumstances, these strategies can be reused across different types of biomedical relationships to learn a heterogeneous knowledge graph in cases where those relationships describe similar physical concepts.
Combining distant supervision with other types of labeling strategies remains an active area of investigation with numerous associated challenges and opportunities.
Overall, semi-supervised learning and weak supervision provide promising results in terms of relationship extraction and future approaches should consider using these paradigms to train machine learning classifiers.

| Dataset | Type of Sentences |
| --- | --- |
| AIMed [@doi:10.1016/j.artmed.2004.07.016] | Protein-Protein Interactions |
| BioInfer [@doi:10.1186/1471-2105-8-50] | Protein-Protein Interactions | 
| LLL [@raw:LLL] | Protein-Protein Interactions |
| IEPA [@raw:IEPA] | Protein-Protein Interactions |
| HPRD5 [@doi:10.1093/bioinformatics/btl616] | Protein-Protein Interactions |
| EU-ADR [@doi:10.1016/j.jbi.2012.04.004] | Disease Gene Associations |
| BeFree [@doi:10.1186/s12859-015-0472-9] | Disease Gene Associations |
| CoMAGC [@doi:10.1186/1471-2105-14-323] | Disease Gene Associations | 
| CRAFT [@doi:10.1186/1471-2105-13-161] | Disease Gene Associations |
| Biocreative V CDR [@doi:10.1093/database/baw068] | Compound induces Disease |
| Biocreative IV ChemProt [@raw:biocreative/chemprot] | Compound Gene Bindings |

Table: A set of publicly available datasets for supervised text mining. {#tbl:supervised-text-datasets}
